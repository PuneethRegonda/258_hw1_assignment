{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdb623d9-cbf7-4833-a66c-9f142c687b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomCNN(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=8192, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, num_classes=7):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(32 * 16 * 16, 64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # [B, 16, 64, 64] -> [B, 16, 32, 32]\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # [B, 32, 16, 16]\n",
    "        x = x.view(x.size(0), -1)             # flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "teacher_model = CustomCNN(num_classes=7)\n",
    "teacher_model.load_state_dict(torch.load(\"custom_cnn.pth\", map_location=device))\n",
    "teacher_model.to(device)\n",
    "teacher_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31321ae9-263f-4815-8fa5-ebc6f23cdddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at WinKawaks/vit-tiny-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([7]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 192]) in the checkpoint and torch.Size([7, 192]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "# ✅ ViT tiny student — for fast distillation\n",
    "student_model = ViTForImageClassification.from_pretrained(\n",
    "    \"WinKawaks/vit-tiny-patch16-224\",\n",
    "    num_labels=7,\n",
    "    id2label={i: str(i) for i in range(7)},\n",
    "    label2id={str(i): i for i in range(7)},\n",
    "    ignore_mismatched_sizes=True  # ✅ fixes classifier shape mismatch\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10e2d5e2-f1f7-4524-b85f-c432bfb8b3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 47163/47163 [13:18<00:00, 59.04 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11791/11791 [03:30<00:00, 56.02 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# ✅ Collect image paths and labels\n",
    "image_paths = glob(\"./data/MedNIST/*/*.jpeg\")\n",
    "labels = [os.path.basename(os.path.dirname(p)) for p in image_paths]\n",
    "label2id = {v: k for k, v in enumerate(sorted(set(labels)))}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "# ✅ Convert to HuggingFace dataset\n",
    "examples = [{\"image\": p, \"label\": label2id[os.path.basename(os.path.dirname(p))]} for p in image_paths]\n",
    "dataset = Dataset.from_list(examples).train_test_split(test_size=0.2)\n",
    "\n",
    "# ✅ Define HuggingFace ViT processor\n",
    "from transformers import ViTImageProcessor\n",
    "processor = ViTImageProcessor.from_pretrained(\"WinKawaks/vit-tiny-patch16-224\")\n",
    "\n",
    "# ✅ Transform function for HuggingFace format\n",
    "def transform(example):\n",
    "    img = Image.open(example[\"image\"]).convert(\"RGB\")\n",
    "    inputs = processor(img, return_tensors=\"pt\")\n",
    "    example[\"pixel_values\"] = inputs[\"pixel_values\"].squeeze()\n",
    "    return example\n",
    "\n",
    "# ✅ Apply transform\n",
    "train_hf = dataset[\"train\"].map(transform)\n",
    "test_hf = dataset[\"test\"].map(transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03da7480-a500-4db5-b291-a322a53b2c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ✅ Collate to handle batch stacking\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([x[\"pixel_values\"] for x in batch])\n",
    "    labels = torch.tensor([x[\"label\"] for x in batch])\n",
    "    return {\"pixel_values\": pixel_values, \"label\": labels}\n",
    "\n",
    "train_hf.set_format(type=\"torch\")\n",
    "test_hf.set_format(type=\"torch\")\n",
    "\n",
    "train_loader = DataLoader(train_hf, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_hf, batch_size=16, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eab88c08-d94b-4173-9759-55b7d9d39f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "class DistillationTrainer(Trainer):\n",
    "    def __init__(self, teacher_model, temperature=4.0, alpha=0.5, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher = teacher_model\n",
    "        self.teacher.eval()\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"label\")\n",
    "        pixel_values = inputs[\"pixel_values\"].to(model.device)\n",
    "        labels = labels.to(model.device)\n",
    "\n",
    "        # Student logits\n",
    "        student_logits = model(pixel_values).logits\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Convert RGB to grayscale like custom CNN input\n",
    "            gray = pixel_values.mean(dim=1, keepdim=True)\n",
    "            teacher_logits = self.teacher(gray)\n",
    "\n",
    "        # Losses\n",
    "        ce_loss = nn.CrossEntropyLoss()(student_logits, labels)\n",
    "        kd_loss = nn.KLDivLoss(reduction=\"batchmean\")(\n",
    "            F.log_softmax(student_logits / self.temperature, dim=1),\n",
    "            F.softmax(teacher_logits / self.temperature, dim=1)\n",
    "        )\n",
    "        loss = self.alpha * ce_loss + (1 - self.alpha) * kd_loss\n",
    "        return (loss, student_logits) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad851542-ff19-47a3-834e-8548fed42ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in ./cmpe258/lib/python3.12/site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in ./cmpe258/lib/python3.12/site-packages (from accelerate) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./cmpe258/lib/python3.12/site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in ./cmpe258/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in ./cmpe258/lib/python3.12/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./cmpe258/lib/python3.12/site-packages (from accelerate) (2.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in ./cmpe258/lib/python3.12/site-packages (from accelerate) (0.30.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./cmpe258/lib/python3.12/site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in ./cmpe258/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./cmpe258/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.2)\n",
      "Requirement already satisfied: requests in ./cmpe258/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./cmpe258/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./cmpe258/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.1)\n",
      "Requirement already satisfied: networkx in ./cmpe258/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./cmpe258/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: setuptools in ./cmpe258/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (78.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./cmpe258/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./cmpe258/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./cmpe258/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./cmpe258/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./cmpe258/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./cmpe258/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./cmpe258/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate -U\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a1d6a3f-a0e4-4450-923f-f0d96d8582a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TrainingArguments\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m training_args = \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./results\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./logs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mno\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremove_unused_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnone\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# disables wandb if used\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:121\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/desktop/USA/2 Sem/258/hw/cmpe258/lib/python3.12/site-packages/transformers/training_args.py:1483\u001b[39m, in \u001b[36mTrainingArguments.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1477\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m version.parse(version.parse(torch.__version__).base_version) == version.parse(\u001b[33m\"\u001b[39m\u001b[33m2.0.0\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fp16:\n\u001b[32m   1478\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1481\u001b[39m     \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1482\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[32m-> \u001b[39m\u001b[32m1483\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m.type != \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1484\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.device.type != \u001b[33m\"\u001b[39m\u001b[33mnpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1485\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.device.type != \u001b[33m\"\u001b[39m\u001b[33mxpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1486\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (get_xla_device_type(\u001b[38;5;28mself\u001b[39m.device) != \u001b[33m\"\u001b[39m\u001b[33mGPU\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1487\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.fp16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fp16_full_eval)\n\u001b[32m   1488\u001b[39m ):\n\u001b[32m   1489\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1490\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1491\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m (`--fp16_full_eval`) can only be used on CUDA or NPU devices or certain XPU devices (with IPEX).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1492\u001b[39m     )\n\u001b[32m   1494\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1495\u001b[39m     \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1496\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[32m   (...)\u001b[39m\u001b[32m   1503\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.bf16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bf16_full_eval)\n\u001b[32m   1504\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/desktop/USA/2 Sem/258/hw/cmpe258/lib/python3.12/site-packages/transformers/training_args.py:1921\u001b[39m, in \u001b[36mTrainingArguments.device\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1917\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1918\u001b[39m \u001b[33;03mThe device used by this process.\u001b[39;00m\n\u001b[32m   1919\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1920\u001b[39m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m1921\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup_devices\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/desktop/USA/2 Sem/258/hw/cmpe258/lib/python3.12/site-packages/transformers/utils/generic.py:54\u001b[39m, in \u001b[36mcached_property.__get__\u001b[39m\u001b[34m(self, obj, objtype)\u001b[39m\n\u001b[32m     52\u001b[39m cached = \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     cached = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/desktop/USA/2 Sem/258/hw/cmpe258/lib/python3.12/site-packages/transformers/training_args.py:1831\u001b[39m, in \u001b[36mTrainingArguments._setup_devices\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1829\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[32m   1830\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[32m-> \u001b[39m\u001b[32m1831\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m   1832\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1833\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPlease run `pip install transformers[torch]` or `pip install accelerate -U`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1834\u001b[39m         )\n\u001b[32m   1835\u001b[39m     AcceleratorState._reset_state(reset_partial_state=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1836\u001b[39m \u001b[38;5;28mself\u001b[39m.distributed_state = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\"  # disables wandb if used\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84139fe5-162e-410e-867e-a345b3309c60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
